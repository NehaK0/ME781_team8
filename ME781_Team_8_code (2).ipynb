{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["#**Source code to detect emotion based on audio clip** \n","Course Project for ME781 \\\\\n","Submitted by : Team 8 \\\\\n","###Team Members : Priyanka, Kartikey, Jasleen, Jigmat, Neha\n","\n","Submitted on : 29-11-2022"],"metadata":{"id":"vKQx0L13kNDY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"ckdDpvUwCmhK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669063620500,"user_tz":-330,"elapsed":3917,"user":{"displayName":"Priyanka Chaudhary","userId":"16174346328243641930"}},"outputId":"69caecf8-0d64-43e5-9cd7-d1e7fa5a3cdb"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.11.0)\n","Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.21)\n"]}],"source":["# Import Libraries\n","import glob\n","import os\n","\n","import numpy as np\n","import soundfile\n","import librosa\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import classification_report\n","from sklearn.metrics import confusion_matrix\n","\n","!pip install soundfile"]},{"cell_type":"code","source":["!pip install gradio"],"metadata":{"id":"unQnQGS_A3Xi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Import data from google drive\n","from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5eNzJcfPYsgM","executionInfo":{"status":"ok","timestamp":1669063629677,"user_tz":-330,"elapsed":4920,"user":{"displayName":"Priyanka Chaudhary","userId":"16174346328243641930"}},"outputId":"72ecb36b-a301-4fbb-b830-c4e71296eb38"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","source":["# Extract features from audio file\n","# Features considered here: MFCC, Chroma, MEL Spectrogram Frequency, Contrast, Tonnetz\n","# These features are the dataset for machine learning model\n","\n","def extract_feature(file_name, **kwargs):\n","    \n","    chroma = kwargs.get(\"chroma\")\n","    contrast = kwargs.get(\"contrast\")\n","    mfcc = kwargs.get(\"mfcc\")\n","    mel = kwargs.get(\"mel\")\n","    tonnetz = kwargs.get(\"tonnetz\")\n","    \n","    with soundfile.SoundFile(file_name) as audio_clip:\n","        X = audio_clip.read(dtype=\"float32\")\n","        sound_fourier = np.abs(librosa.stft(X))   # Conducting short time fourier transform of audio clip\n","        result = np.array([])\n","        \n","        if mfcc:\n","            mfccs = np.mean(librosa.feature.mfcc(y=X, sr=audio_clip.samplerate, n_mfcc=40).T, axis=0)\n","            result = np.hstack((result, mfccs))\n","        if chroma:\n","            chroma = np.mean(librosa.feature.chroma_stft(S=sound_fourier, sr=audio_clip.samplerate).T, axis=0)\n","            result = np.hstack((result, chroma))\n","        if mel:\n","            mel = np.mean(librosa.feature.melspectrogram(X, sr=audio_clip.samplerate).T, axis=0)\n","            result = np.hstack((result, mel))\n","        if contrast:\n","            contrast = np.mean(librosa.feature.spectral_contrast(S=sound_fourier, sr=audio_clip.samplerate).T, axis=0)\n","            result = np.hstack((result, contrast))\n","        if tonnetz:\n","            tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X), sr=audio_clip.samplerate).T, axis=0)\n","            result = np.hstack((result, tonnetz))\n","    return result"],"metadata":{"id":"71vuqNeAKPGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Emotion lookup list \n","emotion_directory = {\"01\": \"neutral\",\"02\": \"calm\",\"03\": \"happy\",\"04\": \"sad\",\"05\": \"angry\",\"06\": \"fearful\",\"07\": \"disgust\",\"08\": \"surprised\"} # all emotions in RAVDESS dataset\n","emotion_allow = {\"angry\",\"sad\",\"neutral\",\"happy\"} # Emotions considered for present model\n","\n","# Function to load entire data and split into datasets for training and testing the model\n","def load_data(test_size=0.25):\n","    X, y = [], []\n","    try :\n","      for file in glob.glob(\"/content/gdrive/MyDrive/ME781_Project/Dataset/Actor_*/*.wav\"):\n","          clipname = os.path.basename(file) # get audio file name\n","          emotion = emotion_directory[clipname.split(\"-\")[2]] # get label of emotion\n","          \n","          if emotion not in emotion_allow:\n","              continue\n","          # Now only the audio clips with allowed emotions are filtered\n","          features = extract_feature(file, mfcc=True, chroma=True, mel=True, contrast=True, tonnetz=True) # extract speech features\n","          # Append to dataset\n","          X.append(features)\n","          y.append(emotion)\n","    except :\n","         print('FAIL')\n","         pass\n","    \n","    return train_test_split(np.array(X), y, test_size=test_size, random_state=7) # return training and testing data "],"metadata":{"id":"YKD3H2DcKX67"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Loading Dataset\n","X_train, X_test, y_train, y_test = load_data(test_size=0.25)\n","\n","print(\"Number of features:\", X_train.shape[1])            # number of features used\n","print(\"Number of training samples = \", X_train.shape[0])  # number of samples in training data\n","print(\"Number of testing samples = \", X_test.shape[0])    # number of samples in testing data\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H7DaYmkJRXuC","executionInfo":{"status":"ok","timestamp":1669065135087,"user_tz":-330,"elapsed":1456146,"user":{"displayName":"Priyanka Chaudhary","userId":"16174346328243641930"}},"outputId":"2aecca8f-cce5-40dc-c64a-393abfb16dcb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of features: 193\n","Number of training samples =  1003\n","Number of testing samples =  335\n"]}]},{"cell_type":"markdown","source":["Random Forest"],"metadata":{"id":"eYNjVMw6VLsa"}},{"cell_type":"code","source":["classifier = RandomForestClassifier(n_estimators = 120, random_state = 0) \n","  \n","# Model Training\n","classifier.fit(X_train, y_train)   \n","# Model Testing\n","Prediction = classifier.predict(X_test) \n","\n","# Printing Model Assessment Details\n","var_accuracy = accuracy_score(y_true=y_test,y_pred=Prediction)\n","var_confusion = confusion_matrix(y_test,Prediction)\n","print(\"Accuracy Score: \", var_accuracy)\n","\n","print(\"Confusion Matrix: \")\n","print(var_confusion)\n","\n","print(\"Classification Report:\")\n","print(classification_report(y_test,Prediction)) \n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wtw42bgdVNr5","executionInfo":{"status":"ok","timestamp":1669065489177,"user_tz":-330,"elapsed":977,"user":{"displayName":"Priyanka Chaudhary","userId":"16174346328243641930"}},"outputId":"1452f8f2-3775-488a-eb84-76a5c49814e0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score:  0.7850746268656716\n","Confusion Matrix: \n","[[84 16  0  4]\n"," [13 78  0  5]\n"," [ 0  3 32 15]\n"," [ 3 11  2 69]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","       angry       0.84      0.81      0.82       104\n","       happy       0.72      0.81      0.76        96\n","     neutral       0.94      0.64      0.76        50\n","         sad       0.74      0.81      0.78        85\n","\n","    accuracy                           0.79       335\n","   macro avg       0.81      0.77      0.78       335\n","weighted avg       0.80      0.79      0.79       335\n","\n"]}]},{"cell_type":"markdown","source":["Support Vector Machine"],"metadata":{"id":"5yVgnBtTKJnV"}},{"cell_type":"code","source":["from sklearn.svm import SVC \n","\n","# Model Training\n","fitted_model = SVC(kernel = 'linear', C = 1).fit(X_train, y_train)  \n","# Model Testing\n","Prediction = fitted_model.predict(X_test)\n","\n","# Printing Model Assessment Details\n","var_accuracy = accuracy_score(y_true=y_test,y_pred=Prediction)\n","var_confusion = confusion_matrix(y_test,Prediction)\n","print(\"Accuracy Score: \", var_accuracy)\n","\n","print(\"Confusion Matrix: \")\n","print(var_confusion)\n","\n","print(\"Classification Report:\")\n","print(classification_report(y_test,Prediction)) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-B2Wlemd8_Lb","executionInfo":{"status":"ok","timestamp":1669065515936,"user_tz":-330,"elapsed":9438,"user":{"displayName":"Priyanka Chaudhary","userId":"16174346328243641930"}},"outputId":"4c533ff6-1d30-4c77-df1b-3aff9af182d3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score:  0.7373134328358208\n","Confusion Matrix: \n","[[83 15  1  5]\n"," [ 9 79  3  5]\n"," [ 3  6 24 17]\n"," [ 4 11  9 61]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","       angry       0.84      0.80      0.82       104\n","       happy       0.71      0.82      0.76        96\n","     neutral       0.65      0.48      0.55        50\n","         sad       0.69      0.72      0.71        85\n","\n","    accuracy                           0.74       335\n","   macro avg       0.72      0.70      0.71       335\n","weighted avg       0.74      0.74      0.73       335\n","\n"]}]},{"cell_type":"markdown","source":["Decision Tree based Classification"],"metadata":{"id":"2isKR9WnKMjh"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier \n","\n","# Model Training\n","fitted_model = DecisionTreeClassifier(max_depth = 7,random_state=0).fit(X_train, y_train)  \n","# Model Testing\n","Prediction = fitted_model.predict(X_test) \n","\n","# Printing Model Assessment Details\n","var_accuracy = accuracy_score(y_true=y_test,y_pred=Prediction)\n","var_confusion = confusion_matrix(y_test,Prediction)\n","print(\"Accuracy Score: \", var_accuracy)\n","\n","print(\"Confusion Matrix: \")\n","print(var_confusion)\n","\n","print(\"Classification Report:\")\n","print(classification_report(y_test,Prediction))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RsEBZww39H0c","executionInfo":{"status":"ok","timestamp":1669066559584,"user_tz":-330,"elapsed":12,"user":{"displayName":"Priyanka Chaudhary","userId":"16174346328243641930"}},"outputId":"4c2bcd36-b321-4300-ea02-36b8556abb46"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy Score:  0.6208955223880597\n","Confusion Matrix: \n","[[76 19  3  6]\n"," [24 55  6 11]\n"," [ 1  4 26 19]\n"," [11  8 15 51]]\n","Classification Report:\n","              precision    recall  f1-score   support\n","\n","       angry       0.68      0.73      0.70       104\n","       happy       0.64      0.57      0.60        96\n","     neutral       0.52      0.52      0.52        50\n","         sad       0.59      0.60      0.59        85\n","\n","    accuracy                           0.62       335\n","   macro avg       0.61      0.61      0.61       335\n","weighted avg       0.62      0.62      0.62       335\n","\n"]}]},{"cell_type":"markdown","source":["# Deployed App using Gradio\n","\n","The best working algorithm, i.e., Random Forest Classifier was used to deploy the trained model for classifying any inputted audio."],"metadata":{"id":"RxyomPvU_Xlz"}},{"cell_type":"code","source":["import gradio as gr\n","import wave\n","import matplotlib.pyplot as plt\n","\n","def emotion_predict(input):\n","  input_features = extract_feature(input, mfcc=True, chroma=True, mel=True, contrast=True, tonnetz=True)\n","  rf_prediction = classifier.predict(input_features.reshape(1,-1))\n","  if rf_prediction == 'happy':\n","    return 'Happy 😎'\n","  elif rf_prediction == 'neutral':\n","    return 'Neutral 😐'\n","  elif rf_prediction == 'sad':\n","    return 'Sad 😢'\n","  else:\n","    return 'Angry 😤'\n","  \n","\n","def plot_fig(input):\n","  wav = wave.open(input, 'r')\n","\n","  raw = wav.readframes(-1)\n","  raw = np.frombuffer(raw, \"int16\")\n","  sampleRate = wav.getframerate()\n","\n","  Time = np.linspace(0, len(raw)/sampleRate, num=len(raw))\n","\n","  fig = plt.figure()\n","\n","  plt.rcParams[\"figure.figsize\"] = (50,15)\n","\n","  plt.title(\"Waveform Of the Audio\", fontsize=25)\n","\n","  plt.xticks(fontsize=15)\n","\n","  plt.yticks(fontsize=15)\n","\n","  plt.ylabel(\"Amplitude\", fontsize=25)\n","\n","  plt.plot(Time, raw, color='red')\n","\n","  return fig\n","\n","\n","with gr.Blocks() as app:\n","  gr.Markdown(\n","        \"\"\"\n","    # Speech Emotion Detector 🎵😍\n","    This application classifies inputted audio 🔊 according to the verbal emotion into four categories:\n","    1. Happy 😎\n","    2. Neutral 😐\n","    3. Sad 😢\n","    4. Angry 😤\n","    \"\"\"\n","  )\n","  with gr.Tab(\"Record Audio\"):\n","    record_input = gr.Audio(source=\"microphone\", type=\"filepath\")\n","        \n","    with gr.Accordion(\"Audio Visualization\", open=False):\n","      gr.Markdown(\n","          \"\"\"\n","      ### Visualization will work only after Audio has been submitted\n","      \"\"\"\n","      )    \n","      plot_record = gr.Button(\"Display Audio Signal\")\n","      plot_record_c = gr.Plot(label='Waveform Of the Audio')\n","    \n","    record_button = gr.Button(\"Detect Emotion\")\n","    record_output = gr.Text(label = 'Emotion Detected')\n","\n","  with gr.Tab(\"Upload Audio File\"):\n","    gr.Markdown(\n","        \"\"\"\n","    ## Uploaded Audio should be of .wav format\n","    \"\"\"\n","    )\n","\n","    upload_input = gr.Audio(type=\"filepath\")\n","\n","    with gr.Accordion(\"Audio Visualization\", open=False):\n","      gr.Markdown(\n","          \"\"\"\n","      ### Visualization will work only after Audio has been submitted\n","      \"\"\"\n","      )\n","      plot_upload = gr.Button(\"Display Audio Signal\")\n","      plot_upload_c = gr.Plot(label='Waveform Of the Audio')\n","\n","    upload_button = gr.Button(\"Detect Emotion\")\n","    upload_output = gr.Text(label = 'Emotion Detected')\n","    \n","  record_button.click(emotion_predict, inputs=record_input, outputs=record_output)\n","  upload_button.click(emotion_predict, inputs=upload_input, outputs=upload_output)\n","  plot_record.click(plot_fig, inputs=record_input, outputs=plot_record_c)\n","  plot_upload.click(plot_fig, inputs=upload_input, outputs=plot_upload_c)\n","\n","app.launch()"],"metadata":{"id":"gfkgnpgk_73y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### The same app was deployed into a permanent hosting site on this link:\n","\n","https://huggingface.co/spaces/workspace/Speech-Emotion-Detector"],"metadata":{"id":"dkRcOZYsAZBZ"}}]}